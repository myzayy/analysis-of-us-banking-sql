# Analysis of US Banking Institutions & Economic Trends 

This repository presents an end-to-end ELT (Extract, Load, Transform) project implemented in Snowflake. The goal was to transform raw, unstructured FDIC banking data into a structured Star Schema, which allows for analysis of the financial stability, geography, and historical growth of US banks.  

## 1. Introduction and Data Source 

The project analyses the financial reports of US banking institutions. The main objectives were to determine: 

* The ratio between bank assets and deposits. 

* Capital in different US states. 

* Historical trends in the creation of new banks (1800–2025). 

* The largest financial institutions. 

**Key Dashboard:**

Below is the final analytical dashboard containing all key metrics.


![Final Dashboard](img/dashboard.jpg)

Source Data: The dataset comes from the Snowflake Marketplace "Banking Analytics Bundle". Originally, the data was unstructured and denormalized like a single flat table, with columns such as:  

*  `MSA` (Bank Name)  
* `STALP` (State)  
* `ADDRESS` (Total Assets - stored as text)  
* `DEPDOM` (Total Deposits - stored as text)  
* `EQ` (Founding Date - stored as text)


### 1.1 Data architecture (ERD)


**Raw data (source):**

The source data existed in a ‘flat’ format typical for raw extracts, which required cleaning.

![Source ERD](img/erd_schema.jpg)

*Figure 1: Source Entity-Relationship Diagram (Raw Flat Structure)*

---

## 2. Dimensional Model

To support efficient analytics, a Star Schema was designed. The model consists of one Fact table and three Dimension tables:

* `FACT_BANK_PERFORMANCE`: The central table containing metrics (Assets, Deposits) and foreign keys. It also includes calculated window functions like `Rank_In_State`.
* `DIM_BANK_DETAILS`: Descriptive attributes of the bank (Name, Website).
* `DIM_GEO`: Geographic hierarchy (State code, City name).
* `DIM_DATE`: Calendar attributes taken from the establishment date (Year, Quarter, Month).

![Star Schema](img/star_schema.jpg)

*Figure 2: Star Schema Optimized for Analytics)*

---

## 3. ELT Process in Snowflake

The ELT Pipeline consists of three stages: ***Staging,** **Transormation** and **Loading***.

### 3.1 Extract & Load (Staging)

The data was extracted from the raw database `BANKING_RAW` and loaded into an intermediate table (`STAGING.stg_fdic_institutions`). At this stage, critical data was cleaned:
* Filtered out records with *Null* names.
* Applied `TRY_TO_NUMBER` in **filtering logic** (`WHERE` condition) to exclude invalid or non-numeric asset records before loading.

```sql
CREATE OR REPLACE TABLE stg_fdic_institutions (

    stg_id INT IDENTITY(1,1) PRIMARY KEY, 
    bank_name   STRING,
    city        STRING,
    state       STRING,
    asset_amt   INT, -- money
    deposit_amt INT,
    est_date    DATE,
    website     STRING
);

INSERT INTO stg_fdic_institutions (bank_name, city, state, asset_amt, deposit_amt, est_date, website)
SELECT 
    MSA,        -- bank name
    CONSERVE,   -- city
    STALP,      -- state
    ADDRESS, -- assets
    DEPDOM,  -- deposits
    EQ,        -- date of found
    WEBADDR     -- site
FROM BANKING_RAW.INSIGHTS.FDIC_INSTITUTIONS
WHERE MSA IS NOT NULL 
  AND TRY_TO_NUMBER(ADDRESS) > 0;

```

### 3.2 Transform (Dimension & Fact Creation)

SQL logic was used to fill in the star schema.

**Dimensions** were created using `DISTINCT` selects to ensure there are no duplicate records (uniqueness).
**Fact Table** was generated by linking dimensions via `JOIN` operations. **Window Functions** was also applied  (like `RANK` and `AVG`) to calculate advanced metrics directly inside the database effectively.

```sql
 RANK() OVER (PARTITION BY s.state ORDER BY s.asset_amt DESC),
    AVG(s.asset_amt) OVER (PARTITION BY s.state)
```


