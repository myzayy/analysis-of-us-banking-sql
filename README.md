# Analysis of US Banking Institutions & Economic Trends 

This repository presents an end-to-end ELT (Extract, Load, Transform) project implemented in Snowflake. The goal was to transform raw, unstructured FDIC banking data into a structured Star Schema, which allows for analysis of the financial stability, geography, and historical growth of US banks.  

## 1. Introduction and Data Source 

The project analyses the financial reports of US banking institutions. The main objectives were to determine: 

* The ratio between bank assets and deposits. 

* Capital in different US states. 

* Historical trends in the creation of new banks (1800–2025). 

* The largest financial institutions. 

**Key Dashboard:**

Below is the final analytical dashboard containing all key metrics.


![Final Dashboard](img/dashboard.jpg)

Source Data: The dataset comes from the Snowflake Marketplace "Banking Analytics Bundle". Originally, the data was unstructured and denormalized like a single flat table, with columns such as:  

*  `MSA` (Bank Name)  
* `STALP` (State)  
* `ADDRESS` (Total Assets - stored as text)  
* `DEPDOM` (Total Deposits - stored as text)  
* `EQ` (Founding Date - stored as text)


### 1.1 Data architecture (ERD)


**Raw data (source):**

The source data existed in a ‘flat’ format typical for raw extracts, which required cleaning.

![Source ERD](img/erd_schema.jpg)

*Figure 1: Source Entity-Relationship Diagram (Raw Flat Structure)*

---

## 2. Dimensional Model

To support efficient analytics, a Star Schema was designed. The model consists of one Fact table and three Dimension tables:

* `FACT_BANK_PERFORMANCE`: The central table containing metrics (Assets, Deposits) and foreign keys. It also includes calculated window functions like `Rank_In_State`.
* `DIM_BANK_DETAILS`: Descriptive attributes of the bank (Name, Website). **(SCD Type 1 - updates overwrite previous values)**
* `DIM_GEO`: Geographic hierarchy (State code, City name). **(SCD Type 0 - static geographic data)**
* `DIM_DATE`: Calendar attributes taken from the establishment date. **(SCD Type 0 - fixed calendar data)**

![Star Schema](img/star_schema.jpg)

*Figure 2: Star Schema Optimized for Analytics*

---

## 3. ELT Process in Snowflake

The ELT Pipeline consists of three stages: ***Staging,** **Transormation** and **Loading***.

### 3.1 Extract & Load (Staging)

The data was extracted from the raw database `BANKING_RAW` and loaded into an intermediate table (`STAGING.stg_fdic_institutions`). At this stage, critical data was cleaned:
* Filtered out records with *Null* names.
* Applied `TRY_TO_NUMBER` in **filtering logic** (`WHERE` condition) to exclude invalid or non-numeric asset records before loading.

```sql
CREATE OR REPLACE TABLE stg_fdic_institutions (

    stg_id INT IDENTITY(1,1) PRIMARY KEY, 
    bank_name   STRING,
    city        STRING,
    state       STRING,
    asset_amt   INT, -- money
    deposit_amt INT,
    est_date    DATE,
    website     STRING
);

INSERT INTO stg_fdic_institutions (bank_name, city, state, asset_amt, deposit_amt, est_date, website)
SELECT 
    MSA,        -- bank name
    CONSERVE,   -- city
    STALP,      -- state
    ADDRESS, -- assets
    DEPDOM,  -- deposits
    EQ,        -- date of found
    WEBADDR     -- site
FROM BANKING_RAW.INSIGHTS.FDIC_INSTITUTIONS
WHERE MSA IS NOT NULL 
  AND TRY_TO_NUMBER(ADDRESS) > 0;

```

### 3.2 Transform (Dimension & Fact Creation)

SQL logic was used to fill in the star schema.

**Dimensions** were created using `DISTINCT` selects to ensure there are no duplicate records (uniqueness).
**Fact Table** was generated by linking dimensions via `JOIN` operations. **Window Functions** was also applied  (like `RANK` and `AVG`) to calculate advanced metrics directly inside the database effectively.

```sql
 -- DIMENSION
USE SCHEMA DWH;

-- geography
CREATE OR REPLACE TABLE DIM_GEO (
    geo_id INT IDENTITY(1,1) PRIMARY KEY,
    state_code STRING,
    city_name STRING
);

INSERT INTO DIM_GEO (state_code, city_name)
SELECT DISTINCT
    state,
    city
FROM STAGING.STG_FDIC_INSTITUTIONS WHERE state IS NOT NULL
ORDER BY state, city;


--Bank details
CREATE OR REPLACE TABLE DIM_BANK_DETAILS (
    bank_id INT PRIMARY KEY,
    name STRING,
    website STRING
);

INSERT INTO DIM_BANK_DETAILS (bank_id, name, website)
SELECT
    stg_id,
    bank_name,
    website
FROM STAGING.STG_FDIC_INSTITUTIONS;

--Date
CREATE OR REPLACE TABLE DIM_DATE (
    date_id INT IDENTITY(1,1) PRIMARY KEY,
    full_date DATE,
    year INT,
    month INT,
    quarter INT
);

INSERT INTO DIM_DATE (full_date, year, month, quarter)
SELECT
    est_date,
    YEAR(est_date),
    MONTH(est_date),
    QUARTER(est_date)
FROM STAGING.STG_FDIC_INSTITUTIONS WHERE est_date IS NOT NULL
GROUP BY EST_DATE
ORDER BY EST_DATE;

-- fact

CREATE OR REPLACE TABLE fact_bank_perfomance (
    fact_id INT IDENTITY(1, 1) PRIMARY key,
    bank_id_fk INT,
    geo_id_fk INT,
    established_date_id_fk INT,
    total_assets INT,
    total_deposits INT,
    rank_in_state INT,
    avg_state_assets INT
);

INSERT INTO fact_bank_perfomance (
    bank_id_fk, geo_id_fk, established_date_id_fk, total_assets, total_deposits, rank_in_state, avg_state_assets
)
SELECT
    b.bank_id,
    g.geo_id,
    d.date_id,
    s.asset_amt,
    s.deposit_amt,
    RANK() OVER (PARTITION BY s.state ORDER BY s.asset_amt DESC), -- window function
    AVG(s.asset_amt) OVER (PARTITION BY s.state)
FROM
    STAGING.stg_fdic_institutions s
JOIN DIM_BANK_DETAILS b ON s.stg_id = b.bank_id -- join operations
LEFT JOIN DIM_GEO g ON s.state = g.state_code and s.city = g.city_name
LEFT JOIN DIM_DATE d ON s.est_date = d.full_date;

```

**Cleanup:** After successfully loading the data into the Data Warehouse, the temporary staging table is removed to optimize storage.

```sql
USE SCHEMA STAGING;
DROP TABLE IF EXISTS stg_fdic_institutions;
```

## 4. Data Visualization & Insights

The final dataset was visualized to answer key business questions.

### Chart 1: Top 10 banks by total assets
Identify the largest financial players in the market. Query logic: combine total assets by bank name.

```sql
SELECT b.name as Bank_name,
SUM(f.total_assets) as Total_assets
From FACT_BANK_PERFOMANCE f
JOIN dim_bank_details b ON f.bank_id_fk = b.bank_id
GROUP BY b.name ORDER BY TOTAL_ASSETS DESC LIMIT 10;
```
![Chart 1](img/10_richest_banks.jpg)

*Figure 3: Top 10 Richest Banks*

### Chart 2: Assets vs. Deposits Correlation
Analyzing the relationship between a bank's size and its deposit volume. 

```sql
SELECT f.total_assets AS Assets,
    f.total_deposits AS Deposits,
    g.state_code AS State
FROM fact_bank_perfomance f
JOIN dim_geo g ON f.geo_id_fk = g.geo_id
WHERE f.total_assets < 100000
LIMIT 1000;
```

![Chart 2](img/assets_vs_deposits.jpg)

*Figure 4: Assets VS Deposits*

### Chart 3: Historical trends in the creation of new banks (1800–2025)
Tracking the expansion of the US banking system over two centuries.

```sql
SELECT d.year AS Year_of_Found, COUNT(f.bank_id_fk) AS New_Banks_account
FROM fact_bank_perfomance f
JOIN dim_date d ON f.established_date_id_fk = d.date_id
WHERE d.year >= 1800 and d.year <= 2025
GROUP BY d.year
ORDER By d.year;
```

![Chart 3](img/bank_system_history.jpg)

*Figure 5: Bank System History*

### Chart 4: Geographic Analysis
Ranking states by total financial assets held by their banks.

```sql
SELECT 
    g.state_code AS State,
    SUM(f.total_assets) AS Total_Assets
FROM fact_bank_perfomance f
JOIN DIM_GEO g ON f.geo_id_fk = g.geo_id
GROUP BY g.state_code
ORDER BY Total_Assets DESC
LIMIT 15;
```

![Chart 4](img/finance_map_of_usa.jpg)

*Figure 6: Finance map of USA*

### Chart 5: 10 states with the largest average bank size
This chart shows the states with the largest average bank assets. 

```sql
SELECT DISTINCT g.state_code AS State,
f.avg_state_assets AS Average_bank_size
FROM fact_bank_perfomance f
JOIN dim_geo g ON f.geo_id_fk = g.geo_id
order by Average_bank_size DESC
LIMIT 10;
```
![Chart 5](img/10_states_by_avg_bank_size.jpg)

*Figure 7: 10 States by average bank size*



Author:

Project by: Maksym Zaiets

Tools: Snowflake, SQL, MySQL Workbench
